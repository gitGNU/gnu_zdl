#!/bin/bash -i
#
# ZigzagDownLoader (ZDL)
# 
# This program is free software: you can redistribute it and/or modify it 
# under the terms of the GNU General Public License as published 
# by the Free Software Foundation; either version 3 of the License, 
# or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, 
# but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY 
# or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License 
# along with this program. If not, see http://www.gnu.org/licenses/. 
# 
# Copyright (C) 2012
# Free Software Foundation, Inc.
# 
# For information or to collaborate on the project:
# https://savannah.nongnu.org/projects/zdl
# 
# Gianluca Zoni
# http://inventati.org/zoninoz
# zoninoz@inventati.org
# 

log=0
n=32
[ -e "/cygdrive" ] && n=10
multi=0
prog=`basename $0`
PROG=`echo $prog | tr a-z A-Z`
path_tmp=".${prog}_tmp"
file_log="${prog}_log.txt"
conf="$HOME/.${prog}rc"
file_data="$HOME/.${prog}.data"
uploaded=0
quit="sì"
url_update="http://inventati.org/zoninoz/html/upload/files/zdl"
max_waiting=40
mkdir -p "$path_tmp"

#user_agent="Mozilla/5.0 (X11; Linux x86_64; rv:10.0.5) Gecko/20100101 Firefox/10.0.5 Iceweasel/10.0.5"

pid_prog=`ps |grep "$prog" |awk '{ print($1) }'|sed -n "1p"`
pid_dl=1   #$pid_prog

rm -f $path_tmp/links_loop.txt

function old_ip {
	[ "$url" != "${url//mediafire.}" ] && host=mediafire
	[ "$url" != "${url//uploaded.}" ] && host=uploaded
	[ "$url" != "${url//easybytez.}" ] && host=easybytez
#	[ "$url" != "${url//sharpfile.}" ] && host=sharpfile
	for key in $( seq 0 (( ${#newip[*]}-- )) ); do
		[ "${newip[$key]}" == "$host" ] && unset newip[$key]
	done
	http_proxy=$old_proxy
}

function print_c {
  case "$1" in
    1)
       echo -n -e '\e[1;32m' #verde
    ;;
    2)
       echo -n -e '\e[0;33m' #giallo
    ;;	
    3)
       echo -n -e '\e[1;31m' #rosso
    ;;	
  esac
  echo -n $2
  echo -e '\e[0m'
}

function separator {
	#COLUMNS=$( tput cols ) 2>/dev/null
	if [ ! -z "$COLUMNS" ];then
		for column in `seq 1 $COLUMNS`; do echo -n -e "\e[1;34m"$1'\e[0m' ; done
	fi
}

function header {
	echo -n -e "\e[1;34m"ZigzagDownLoader [$PROG]"\e[0m\n"
}


function links_loop { #usage with op=+|- : links $op $link
	op="$1"
	link="$2"
	if [ "$op" == "+" ]; then
		[ ! -z "$link" ] && echo "$link" >> $path_tmp/links_loop.txt
	elif [ "$op" == "-" ]; then
		#togliere indirizzo da loop
		if [ -f "$path_tmp/links_loop.txt" ]; then
			lines=`cat "$path_tmp/links_loop.txt" |wc -l`
			for line in `seq 1 $lines`; do
				lnk=`cat $path_tmp/links_loop.txt |sed -n "${line}p"` 
				[ "${link[*]//$lnk}" == "${link[*]}" ] && echo "$lnk" >> $path_tmp/links_loop2.txt
			done
			[ -f $path_tmp/links_loop2.txt ] && mv $path_tmp/links_loop2.txt $path_tmp/links_loop.txt
			links=`cat $path_tmp/links_loop.txt`
			[ -z "$links" ] && rm $path_tmp/links_loop.txt
		fi
	fi
	
}

function pseudo_captcha {
	j=0
	for cod in ${ascii_dec[*]}; do 
		captcha[$j]=`printf "\x$(printf %x $cod)"`
		(( j++ ))
	done
}

function check_ip {
	if [ "${newip[*]}" != "${newip[*]//$1}" ]; then 
		if [ ! -f "$file_data" ]; then
			new_ip
		else
			[ "$multi" == "1" ] && new_ip
			[ "$multi" == "0" ] && new_ip_router
		fi
	fi
}

function my_ip {
	myip=`wget -q -O - -t 1 -T 20 checkip.dyndns.org|sed -e 's/.*Current IP Address: //' -e 's/<.*$//'`
}

function new_ip_router {
	if [ -f "$file_data" ]; then
		USER=`cat $file_data |awk '{ print($1) }'`
		PASSWD=`cat $file_data |awk '{ print($2) }'`
		print_c 1 "Cambio indirizzo IP..."
		wget --http-passwd=$PASSWD --http-user=$USER 192.168.0.1/stanet.stm  -O - &>/dev/null
		wget --http-passwd=$PASSWD --http-user=$USER --post-data="disconnect=1" 192.168.0.1/cgi-bin/statusprocess.exe -O - &>/dev/null
	else
		echo
		print_c 3 "Funzione di cambio indirizzo IP via router disattivata: non esiste il file di configurazione $HOME/.${prog}.data"
	fi
}

function noproxy {
		unset http_proxy
		export http_proxy
}

function new_ip {
	maxspeed=0
	minspeed=25
	unset close unreached speed
	rm -f "$path_tmp/proxy.tmp"
	#proxy_types=( "Transparent" "Anonymous" "Elite" )
	while true; do
		proxy=""
		#### tipi di proxy: Anonymous Transparent Elite
		proxy_types=( "Transparent" )
		#[ "$url" != "${url//mediafire.}" ] && proxy_types=( "Elite" )
		[ "$url" != "${url//uploaded.}" ] && proxy_types=( "Anonymous" "Elite" )
		#[ "$url" != "${url//shareflare.}" ] && proxy_types=( "Transparent" )
		#[ "$url" != "${url//easybytez.}" ] && proxy_types=( "Transparent" )
		ptypes="${proxy_types[*]}"
		print_c 1 "Aggiorna proxy (${ptypes// /, }):"
		old=$http_proxy
		
		noproxy
		line=1
		while [ "$proxy" == "" ] ; do		
			#rm -f "$path_tmp/proxy.tmp"
			if [ ! -f "$path_tmp/proxy.tmp" ]; then
				wget -q -t 1 -T 20 --user-agent="Anonimo" http://www.ip-adress.com/proxy_list/ -O "$path_tmp/proxy.tmp" &>/dev/null
				rm -f "$path_tmp/proxy2.tmp"
			fi
			
			for proxy_type in ${proxy_types[*]}; do
				less "$path_tmp/proxy.tmp"|grep "Proxy_Details" |grep "${proxy_type}" >> "$path_tmp/proxy2.tmp"
			done
			
			max=`wc -l "$path_tmp/proxy2.tmp" | awk '{ print($1) }'`
			#cat "$path_tmp/proxy2.tmp"
			string_line=`cat "$path_tmp/proxy2.tmp" |sed -n "${line}p"`
			
			proxy="${string_line#*Proxy_Details\/}"
			[ "$proxy" != "${proxy%:Anonymous*}" ] && proxy_type="Anonymous"
			[ "$proxy" != "${proxy%:Transparent*}" ] && proxy_type="Transparent"
			[ "$proxy" != "${proxy%:Elite*}" ] && proxy_type="Elite"
			proxy="${proxy%:${proxy_type}*}"
			
			z=$(( ${#proxy_done[*]}-1 ))
			if (( $z<0 )) || [ "$z" == "" ]; then z=0 ; fi
			
			for p in `seq 0 $z`; do
				if [ "${proxy_done[$p]}" == "$proxy" ]; then
					proxy=""
				fi
			done
			
			if [ "$string_line" == "" ]; then
					echo -n -e "."
					sleep 3
					(( search_proxy++ ))
					[ $search_proxy == 100 ] && print_c 3 "Finora nessun proxy disponibile: tentativo con proxy disattivato" && noproxy && close=true && break
			fi
			if [ $line == $max ] || [ "$string_line" == "" ]; then
				rm -f "$path_tmp/proxy.tmp"
				line=0
			fi
			(( line++ ))
			[ "$proxy" != "" ] && [ "${proxy_done[*]}" == "${proxy_done[*]//$proxy}" ] && proxy_done[${#proxy_done[*]}]="$proxy"
		done
		unset search_proxy
		[ ! -z $close ] && break
		http_proxy=$proxy
		export http_proxy
		echo -n "Proxy: $http_proxy ($proxy_type)"
		echo
		unset myip
		#my_ip
		#echo "Nuovo IP: $myip"
		print_c 2 "Test velocità di download:"
		index=0
		while (( $index<3 )); do
			index=${#speed[*]}
			speed[$index]=`wget -t 1 -T $max_waiting -O /dev/null "$indirizzo" 2>&1 | grep '/s'`  #'\([0-9.]\+ [KM]B/s\)'`
			speed[$index]="${speed[$index]#*'('}"
			show_speed="${speed[$index]## }"
			show_speed="${show_speed%%)*}"
			if [ "${speed[$index]}" != "${speed[$index]% B/s*}" ]; then
				speed[$index]="0"
			elif [ "${speed[$index]}" != "${speed[$index]//'KB/s'}" ]; then
				speed[$index]="${speed[$index]%%' '*}"
				speed[$index]="${speed[$index]%','*}"
			elif [ "${speed[$index]}" != "${speed[$index]//'MB/s'}" ]; then
				speed[$index]="${speed[$index]%%' '*}"
				speed[$index]="${speed[$index]//,/.}"
				speed[$index]="${speed[$index]%% *}"
				speed[$index]=`echo "${speed[$index]}*1024" | bc -l`
				speed[$index]="${speed[$index]%'.'*}"
			fi
			if [ "${speed[$index]}" == "" ]; then
				#print_c 3 "Download interrotto"
				#unreached="true"
				#break
				speed[$index]=0
				show_speed=`print_c 3 "0 B/s"`
			fi
			
			echo "$show_speed"
		done 2>/dev/null
		
		if [ -z $unreached ]; then
			
			for k in ${speed[*]}; do  
				if (( $maxspeed<$k )); then 
					maxspeed=$k 
				fi 
			done
			
			if (( $maxspeed<$minspeed )); then
				print_c 3 "La massima velocità di download raggiunta usando il proxy è inferiore a quella minima richiesta ($minspeed KB/s)"
				#unset $proxy
				
			else
				print_c 1 "Massima velocità di download raggiunta usando il proxy $http_proxy: $maxspeed KB/s"
				break
			fi 2>/dev/null
		fi
		unset unreached speed
	done
	unset maxspeed
	echo
	rm -f "$path_tmp/proxy.tmp"
	old_proxy="$proxy"
}

function get_tmps {
	while [ "`less $path_tmp/zdl.tmp |grep \</html`" == "" ]; do
		#print_c 2 "\nAttendi..."
		wget -t 5 -T $max_waiting --retry-connrefused --save-cookies=$path_tmp/cookies.zdl -O "$path_tmp/zdl.tmp" $indirizzo  &>/dev/null
		echo -e "...\c"
	done
}

function input_hidden {
	j=1
	less $tmp | grep input | grep hidden > $path_tmp/data.tmp
	max=`wc -l "$path_tmp/data.tmp" | awk '{ print($1) }'`
	max=$(( $max+1 ))
	
	while [ $j != $max ]; do
		data=`less $path_tmp/data.tmp |sed -n "${j}p"`
		
		name=${data#*name=\"}
		name=${name%%\"*}
		value=${data#*value=\"}
		value=${value%%\"*}
		
		if [ "$name" == "realname" ] || [ "$name" == "fname" ]; then # <--easybytez e sharpfile
			nomefile="$value"
		fi

		if [ "$post_data" == "" ]; then
			post_data="${name}=${value}"
		else
			post_data="${post_data}&${name}=${value}"
		fi
		(( j++ ))
	done
}


function _log {
	if [ $log == 0 ]; then
		echo -e "I file seguenti non sono stati scaricati perchè già presenti nella directory di destinazione o perché non disponibili (forse solo temporaneamente):\n">$file_log
		log=1
	fi
	date >> $file_log
	
	case $1 in
		1)
			echo
			print_c 3  "File $nomefile già presente nella directory di destinazione"  | tee -a $file_log
			echo
			;;
		2)
			echo
			if [ ! -z "$link" ]; then
				link_log=" (link di download: $link) "
			fi
			print_c 3  "$indirizzo --> File ${nomefile}${link_log}non disponibile, riprovo più tardi"  | tee -a $file_log
			echo
			links_loop + "$indirizzo"
			;;
		3)
			echo
			print_c 3  "$indirizzo --> Indirizzo errato o file non disponibile" | tee -a $file_log
			echo
			;;
		4)
			echo
			print_c 3 "Il file $nomefile supera la dimensione consentita dal server per il download gratuito (link: $indirizzo)" | tee -a $file_log
			echo
			;;
		5)
			echo
			print_c 3 "Nessun nome per il file collegato dal seguente link: $link (connessione interrotta: riprovo più tardi)" | tee -a $file_log
			echo
			;;
		6)
			echo
			print_c 3 "$indirizzo --> File $nomefile troppo grande per lo spazio libero in $PWD su $dev: non sarà scaricato." | tee -a $file_log
			echo
			;;
	esac
	
}


function usage {
	print_c 3 "Uso (l'ordine degli argomenti non è importante): `basename $0` [wget|axel] [nomefile] [-m|--multi] [-h|--help] [-c|--configure] [-u|--update] [-clean] [-i|--interactive]"
	echo
	echo -e "$PROG è abilitato per il download da nowdownload, mediafire.com, uploaded.to (ul.to), easybytez.net

$PROG può essere avviato in due modi:

	1) Generando automaticamente un file per la lista dei link per il download (si chiamerà links.txt)
		- apri un terminale ed entra nella directory che dovrà contenere i file scaricati
		- digita il seguente comando e premi invio: $prog 
		- copia i link dei file da scaricare e incollali nel terminale (vai a capo dopo ogni link)
		- premi la chiocciolina \"@\"
		
	2) Utilizzando un file preparato con un editor di testi (andare a capo dopo ogni link)
		- apri un terminale ed entra nella directory che dovrà contenere i file scaricati
		- digita il seguente comando e premi invio: $prog nomefile.txt
		

Per i servizi di sharing seguenti è consigliato l'uso della funzione \"multi\" (aggiungere l'argomento -m alle istruzioni sopra elencate) per procedere con il download in parallelo attraverso l'uso di proxy:
uploaded.to (ul.to), easybytez.net

In caso di interruzione del download (per esempio a causa di disconnessione), i file scaricati attraverso Axel possono riprendere il download dal punto di interruzione (solo se nella cartella che contiene il file scaricato è ancora presente anche un file omonimo con estensione \".st\"). $PROG provvederà automaticamente a recuperare il download o a rieseguirlo da capo. Nel caso in cui anche $PROG è stato terminato, il recupero manuale è possibile riavviando $PROG nella seguente forma:
	$prog NOMEFILE [-m] [axel|wget]

NOMEFILE sta per il nome del file di testo che contiene i link dei file da scaricare. Se il primo download è stato effettuato nel primo modo (vedi sopra, punto 1: generando automaticamente un file per la lista dei link) allora NOMEFILE è links.txt. Gli argomenti \"-m\" e \"axel\" (o \"wget\")  sono facoltativi.

Se i file interrotti sono stati scaricati con Wget, non possono essere recuperati e verranno cancellati dalla directory prima di effettuare il riavvio di $PROG. I servizi di file-sharing seguenti sono abilitati solo per il download con Wget: uploaded.to (ul.to), easybytez.net. Gli altri servizi (nowdownload, mediafire.com) sono abilitati di default per Axel e se interrotti possono essere recuperati e completati (automaticamente da $PROG o manualmente riavviando $PROG nella modalità suggerita qui sopra).

L'argomento [wget|axel], wget oppure axel, consente la scelta del downloader. Axel è un acceleratore di download fortemente consigliato e abilitato di default ma disabilitato per alcuni servizi si filesharing. L'argomento [-c|--configure] consente di configurare il downloader di default, cioè di selezionare Wget al posto di Axel senza dover attivare Wget manualmente adottando l'argomento \"wget\".

L'argomento [-clean] cancella eventuali residui di file temporanei di $PROG nella directory di destinazione, prima di iniziare a processare i link immessi dall'utente.

La funzione [-i|--interactive] permette di visualizzare i download \"vivi\" di $PROG anche da un altro terminale oppure nel caso in cui $PROG è terminato (per vostra volontà, premendo Ctrl+C, oppure accidentalmente). Infatti, i download procedono in background e non muoiono insieme a $PROG (tecnicamente possono essere tutti uccisi \"terminando il terminale\"). Per uccidere (definitivamente oppure per riavviarli automaticamente) uno o più processi già avviati, anche con $PROG perfettamente attivo, da un altro terminale entrare nella directory di destinazione e digitare \"$prog -i\" o \"$prog --interactive\": comparirà un'interfaccia (per ora molto rudimentale) con cui poter interagire con i processi di $PROG.

È possibile (ed è raccomandato) far processare, nella stessa lista, link di mirror diversi per uno stesso file (per esempio: vogliamo scaricare file.part1.rar, file.part2.rar e file.part3.rar e abbiamo copie di questi file in uploaded e in easybytez. Si consiglia di usare tutti i link disponili, sia quelli di easybytez che quelli di uploaded: $PROG processerà tutti i link e scaricherà una sola copia dei file, utilizzando il link migliore).

Per aggiornare $PROG è sufficiente usare l'argomento -u (--update).

$PROG funziona anche su Windows. 
Installazione su Windows in due fasi:
	FASE 1 _ Installazione di Cygwin
		- installatore automatico di Cygwin (serve anche ad aggiornare il sistema emulato e ad installare nuovi pacchetti): http://cygwin.com/setup.exe
		- installare il pacchetto \"wget\"

	FASE 2 _ Installazione di $PROG e di Axel
		- salvare nella cartella C:\\\cygwin il seguente file: http://inventati.org/zoninoz/html/upload/files/install_zdl-cygwin.sh
		- avviare cygwin installato nella fase 1 
		- digitare il seguente comando: /install_zdl-cygwin.sh

Uso di $PROG su Windows: avviare cygwin e utilizzare $PROG nel terminale avviato, come descritto in questa guida.

$PROG è rilasciato con licenza GPL (General Public Licence, v.3 e superiori).

Per informazioni e per collaborare al progetto:
https://savannah.nongnu.org/projects/zdl

Gianluca Zoni (zoninoz)
http://inventati.org/zoninoz
"	
	echo
	echo
	exit 1
}

function install_test {
	
	if [ -z "`which axel 2>/dev/null`" ]; then
		print_c 3 "Installazione automatica non riuscita"
		case $1 in
			pk) echo "$2 non ha trovato il pacchetto di Axel" ;;
			src) echo "Errori nella compilazione o nell'installazione";;
		esac
	fi
	echo
	print_c 2 "<Premi un tasto per continuare>"
	read
}

function install_pk {
	print_c 1 "Installo Axel ..."
	if [ `which apt-get 2>/dev/null` ]; then
		DEBIAN_FRONTEND=noninteractive sudo apt-get --no-install-recommends -q -y install axel || (  echo "Digita la password di root" ; DEBIAN_FRONTEND=noninteractive su -c "apt-get --no-install-recommends -q -y install axel" )
		install_test pk apt-get
	elif [ `which yum 2>/dev/null` ]; then
		sudo yum install axel || ( echo "Digita la password di root" ; su -c "yum install axel" )
		install_test pk yum
	elif [ `which pacman 2>/dev/null` ]; then
		sudo pacman -S axel 2>/dev/null || ( echo "Digita la password di root" ; su -c "pacman -S axel" )
		install_test pk pacman
	else
		install_test
	fi
}

function install_src {
	cd /usr/src
	wget http://alioth.debian.org/frs/download.php/3015/axel-2.4.tar.gz
	tar zxvf axel-2.4.tar.gz
	cd axel-2.4
	
	make
	sudo make install || ( echo "Digita la password di root" ; su -c "make install" )
	make clean
	install_test src
	cd -
}

function configure {
	echo
	print_c 3 "CONFIGURAZIONE DI $PROG"
	echo "Il downloader attuale di $PROG è $downloader"
	echo
	print_c 2 "Scegli il downloader (wget|axel):"
	read dloader
	case $dloader in
		wget) 
			downloader=Wget
		;;
		axel) 
			downloader=Axel
		;;
		*)
			print_c 3 "Downloader non riconosciuto: puoi scegliere solo wget o axel"
			exit 1
		;;
	esac
	echo $downloader > $conf
	echo
	print_c 1 "$PROG scaricherà con $downloader"
	exit
}

function check_freespace {
	mkdir -p $path_tmp
	df > $path_tmp/df.tmp
	
	maxl=`wc -l $path_tmp/df.tmp |awk '{ print($1) }'`
	pattern=`ls -l $PWD`
	pattern="${pattern#*-> }"
	for l in `seq 2 $maxl`; do
		dev=`cat $path_tmp/df.tmp | awk '{ print($6) }' | sed -n "${l}p"`
		if [ "$dev" == "/" ]; then dev="/home" ; fi
		freespace=`cat $path_tmp/df.tmp | awk '{ print($4) }' | sed -n "${l}p"`
		if [ ! -z "$tot_size" ];then 
			t_size=$(( $tot_size/1024 ))
		else
			t_size=0
		fi
		if [ "$pattern" != "${pattern//$dev}" ]; then
			if (( $freespace<5000 )); then
				print_c 3 "Spazio insufficiente sul device. $PROG terminato."
				exit
			elif [ $t_size != 0 ] && (( $freespace<$t_size )); then
#				read -p "STA PER UCCIDERE!"
				kill_dl
				_log 6
			fi
		fi
	done
	unset t_size
}

function interactive {
	
	totf=`ls -1 $path_tmp/*_stdout.tmp 2>/dev/null | wc -l`
	if (( $totf>0 )); then
		if (( $totf>1 )); then 
			nlines=`seq 1 $totf`
		else
			nlines=1
		fi
		unset sel
		n=0
		for i in $nlines; do
			unset name_stdout fname fsize hellip_namef linkdl tot_size dlr pid progress rmfile
			name_stdout=`ls -1 $path_tmp/*_stdout.tmp  2>/dev/null | sed -n "${i}p"`
			
			if [ ! -z "$name_stdout" ]; then 
				
				head -n 8 "$name_stdout" > $path_tmp/head_stdout
	
				namef="${name_stdout//_stdout.tmp}"
				namef="${namef#$path_tmp/}"
				pid=`head -n 1 "$path_tmp/head_stdout"`
				ps au | awk '{print $2}' | while read alive; do
					if [ "$alive" == "$pid" ]; then
						touch $path_tmp/killing
						break
					fi
				done
				
				if [ -f "$path_tmp/killing" ]; then
					rm $path_tmp/killing
					linkdl=`cat "$path_tmp/head_stdout"|grep "link_$prog"`
					linkdl="${linkdl#link_${prog}: }"
		
					dlr=`head -n 3 "$name_stdout" | sed -n '3p'`
			
					if [ "$dlr" == "Wget" ]; then
						tot_size=`cat "$name_stdout" |grep "Content-Length:"`
						tot_size="${tot_size#*Content-Length: }"
						tot_size="${tot_size%%' '*}"
						if [ "$namef" != "${namef//uploaded.hellip.tmp}" ];then
							hellip_namef="$namef"
							
							namef=`cat $path_tmp/${hellip_namef}_stdout.tmp |grep filename`
							namef="${namef#*'filename="'}"
							namef="${namef%'"'*}"
							name_stdout="$path_tmp/${hellip_namef}_stdout.tmp"
						fi
						sleep 3
						unset progress
						progress=`tail -n 1 "$name_stdout"`
					elif [ "$dlr" == "Axel" ]; then
						tot_size=`cat "$path_tmp/head_stdout" |grep 'File size'`
						tot_size="${tot_size#*File size: }"
						tot_size="${tot_size%% *}"
						#sleep 1
						#rm -f "$name_stdout"
						cat $path_tmp/head_stdout > "$name_stdout"
						sleep 2
						unset progress
						progress=`tail -n 2 "$name_stdout" |grep K `
					fi
					
					sel[${#sel[*]}]=$n
					kpid[$n]=$pid
					knamef[$n]="$namef" 
					khellip_namef[$n]="${hellip_namef}"
					kname_stdout[$n]="$name_stdout"
					kdlr[$n]=$dlr
					kprogress[$n]="$progress"
					klinkdl[$n]="$linkdl"
					n=$(( $n+1 ))
				fi
				
			fi
		done
		if [ $n != 0 ]; then
			(( n-- ))
			#show:
			print_c 1 "Processi di download attualmente in vita:"
			separator "="
			for j in `seq 0 $n`; do
				print_c 1 "Numero download attivo: $j"
				[ ! -f "${knamef[$j]}" ] && [ ! -f "${kname_stdout[$j]}" ] && print_c 3 "${kdlr[$j]} sta scaricando a vuoto: ${knamef[$j]} non esiste"
				echo -e "File: ${knamef[$j]}" 
				[ ! -z "${khellip_namef[$j]}" ] && echo "Alias: ${khellip_namef[$j]}"
				echo -e "Downloader: ${kdlr[$j]}\nLink: ${klinkdl[$j]}\nProgresso: ${kprogress[$j]}"
				if [ $i != $totf ]; then 
					separator "-"
				else
					separator "="
				fi
			done
			echo
			print_c 2 "Seleziona i numeri dei download attivi da uccidere, separati da spazi (digita Ctrl+C per uscire subito):"
			read input
			inputs=( $input )
			while [ "$input2" != "d" ] && [ "$input2" != "r" ] && [ "$input2" != "t" ]; do
				print_c 2 "Vuoi che i download selezionati siano terminati definitivamente oppure che siano riavviati automaticamente più tardi?"
				echo
				echo -e "	d) per ucciderli definitivamente (e cancellare il file), digita \"d\"
	r) per riavviarli digita \"r\"
	t) per terminare il programma senza fare niente digita \"t\""
				print_c 2 "Scegli cosa fare: [d | r | t]"
				read input2
				if [ "$input2" == "d" ] || [ "$input2" == "r" ]; then
					for s in ${inputs[*]}; do
						if [ "${sel[*]}" != "${sel[*]//$s}" ]; then
							kill ${kpid[$s]}
							rm -f "${knamef[$s]}" "${khellip_namef[$s]}" "${kname_stdout[$s]}"
						fi
					done
					if [ "$input2" != "d" ]; then
						if [ -f "$path_tmp/links_loop.txt" ]; then
							lines=`cat "$path_tmp/links_loop.txt" |wc -l`
							for line in `seq 1 $lines`; do
								lnk=`cat $path_tmp/links_loop.txt |sed -n "${line}p"` 
								[ "${klinkdl[*]//$lnk}" == "${klinkdl[*]}" ] && echo "$lnk" >> $path_tmp/links_loop2.txt
							done
							[ -f $path_tmp/links_loop2.txt ] && mv $path_tmp/links_loop2.txt $path_tmp/links_loop.txt
						fi
					fi
				elif [ "$input2" != "t" ]; then
					print_c 1  "Nessun processo ucciso"
					exit
				fi
			done
		else
			print_c 1  "Nessun download attivo di $PROG rilevato"
		fi
		unset sel input knamef khellip_namef kname_stdout 
	else
		print_c 1 "Nessun download di $PROG rilevato"
	fi
	exit
}

function kill_dl {
	kill ${pid_dl}
	killed[${#killed[*]}]=${pid_dl}
	unset pid_dl

}

function check_dl {
	totf=`ls -1 $path_tmp/*_stdout.tmp 2>/dev/null | wc -l`
	if (( $totf>0 )); then
		unset name_stdout fname fsize
		print_c 1 "Downloading..."
		separator "="
		unset indirizzo
		if (( $totf>1 )); then 
			nlines=`seq 1 $totf`
		else
			nlines=1
		fi
		for i in $nlines; do
			unset name_stdout fname fsize hellip_namef linkdl tot_size dlr pid progress rmfile
			if [ $multi == 1 ]; then 
				name_stdout=`ls -1 $path_tmp/*_stdout.tmp 2>/dev/null| sed -n "${i}p"`
			else
				name_stdout=`ls -1 $path_tmp/${nomefile}_stdout.tmp | sed -n "${i}p"`
			fi
			
			if [ "$name_stdout" != "" ]; then 
				
				head -n 8 "$name_stdout" > $path_tmp/head_stdout
	
				namef="${name_stdout//_stdout.tmp}"
				namef="${namef#$path_tmp/}"
				pid=`head -n 1 "$path_tmp/head_stdout"`
				linkdl=`cat "$path_tmp/head_stdout"|grep "link_$prog"`
				linkdl="${linkdl#link_${prog}: }"
	
				dlr=`head -n 3 "$name_stdout" | sed -n '3p'`
		
				if [ "$dlr" == "Wget" ]; then
					tot_size=`cat "$name_stdout" |grep "Length:" |tail -n 1`
					tot_size="${tot_size#*Length: }"
					tot_size="${tot_size%%' '*}"
					if [ "$namef" != "${namef//uploaded.hellip.tmp}" ];then
						hellip_namef="$namef"
						
						namef=`cat $path_tmp/${hellip_namef}_stdout.tmp |grep filename`
						namef="${namef#*'filename="'}"
						namef="${namef%'"'*}"
						name_stdout="$path_tmp/${hellip_namef}_stdout.tmp"
					fi
					sleep 3
					unset progress
					progress=`tail -n 1 "$name_stdout"`
				elif [ "$dlr" == "Axel" ]; then
					tot_size=`cat "$path_tmp/head_stdout" |grep 'File size'`
					tot_size="${tot_size#*File size: }"
					tot_size="${tot_size%% *}"
					#sleep 1
					#rm -f "$name_stdout"
					cat $path_tmp/head_stdout > "$name_stdout"
					sleep 2
					unset progress
					progress=`tail -n 2 "$name_stdout" |grep K `
				fi
				#[ "$progress" == "${progress//%}" ] && progress="Download in attesa"
								
				#check pid:
				ps au | awk '{print $2}'  | while read alive; do
					if [ "$alive" == "$pid" ]; then
						touch $path_tmp/alive
						break
					fi
				done
				
				#show:
				sizef=`ls -l "$namef" 2>/dev/null| awk '{ print($5) }'`
				echo "File: $namef - $dlr: $linkdl"
				if [ ! -f $path_tmp/alive ]; then
					progress="Download non attivo"
				fi
				if [ -f "$namef" ] && [ ! -f "${namef}.st" ] && [ "$sizef" == "$tot_size" ];then
					progress="Download completato"
				fi
				echo "$progress"
				if [ $i != $totf ]; then 
					separator "-"
				else
					separator "="
				fi
				
				#check files:	
				if [ ! -f $path_tmp/alive ]; then 
					var_pids="${pids_alive[*]}"
					var_pids="${var_pids//$pid/-}"
					var_pids="${var_pids//-}"
					pids_alive=( $var_pids )
					
					already_there=`cat "$name_stdout"|grep 'already there; not retrieving.'`
					if [ -z "$already_there" ]; then # morto a posteriori = suicida oppure incidentato/completato
						unset already_there
						if [ "${killed[*]}" == "${killed[*]//$pid}" ]; then # incidentato o completato
							fsize=`ls -l "$namef" | awk '{ print($5) }'`
							
							if [ "$fsize" != "$tot_size" ] && [ ! -z "$tot_size" ] && (( $tot_size>0 )); then  #incidentato
								if [ "$hellip_namef" != "$namef" ] && [ "$hellip_namef" != "" ] ; then
									rmfile[${#rmfile[*]}]="$path_tmp/$hellip_namef"
								fi	
								
								[ ! -f "${namef}.st" ] && rmfile[${#rmfile[*]}]="$namef"
								links_loop + "$linkdl"
#								unset nomefile
								checked=1
							
							else #completato
								#rmfile[${#rmfile[*]}]="$name_stdout"
#								unset nomefile
								links_loop - "$linkdl"
								checked=1
							fi
						else #suicida 
							rmfile[${#rmfile[*]}]="$namef"
#							unset nomefile
							checked=1
							killed[${#killed[*]}]=$pid
						fi
					else 
					# morto a priori = completato oppure escluso fin dall'inizio perché il file è presente prima dell'avvio di $PROG ma è sfuggito all'analisi precedente
						#rmfile[${#rmfile[*]}]="$name_stdout"
#						unset nomefile
						checked=1
						links_loop - "$linkdl"
					fi
					
				else
					if [ ! -f "$namef" ] && [ ! -f "$hellip_namef" ]; then
						killed[${#killed[*]}]=$pid
						kill $pid
					elif [ "$pid" != "$pid_dl" ] && [ "$newname" == "$namef" ];then
						return 5
					elif [ "$pid" == "$pid_dl" ] && [ "$newname" == "$namef" ] && [ -f "$hellip_namef" ] && [ -f "$namef" ];then
						rm -f "$hellip_namef"
						kill $pid
					fi 
					rm $path_tmp/alive
					if [ "$multi" == "1" ]; then
#						unset nomefile
						checked=1
					fi
					links_loop - "$linkdl"
					pids_alive[${#pids_alive[*]}]=$pid
				fi
			fi
		done
		if [ "${#rmfile[*]}" != "0" ] && [ ! -z "${#rmfile[*]}" ]; then
			for rmf in `seq 0 $(( ${#rmfile[*]}-1 ))`; do
# 				echo
# 				cat $path_tmp/links_loop.txt
# 				echo
# 				read -p "Cancellazione dei seguenti file: ${rmfile[$rmf]}"
				rm -rf "${rmfile[$rmf]}" 
				unset rmfile[$rmf]
			done
		fi
	else
#		unset nomefile
		checked=1
	fi
	rm -f $path_tmp/alive
}

function check_loop {
	if [ -f "$nomefile" ]; then
		totf=`ls -1 $path_tmp/*_stdout.tmp  2>/dev/null | wc -l`
		if (( $totf>0 )); then
			if (( $totf>1 )); then 
				nlines=`seq 1 $totf`
			else
				nlines=1
			fi
			unset sel
			n=0
			for i in $nlines; do
				unset name_stdout fname fsize hellip_namef linkdl tot_size dlr pid progress rmfile
				name_stdout=`ls -1 $path_tmp/*_stdout.tmp  2>/dev/null | sed -n "${i}p"`
				
				if [ ! -z "$name_stdout" ]; then 
					
					head -n 8 "$name_stdout" > $path_tmp/head_stdout
		
					namef="${name_stdout//_stdout.tmp}"
					namef="${namef#$path_tmp/}"
					dlr=`head -n 3 "$name_stdout" | sed -n '3p'`
			
					if [ "$dlr" == "Wget" ]; then
						tot_size=`cat "$name_stdout" |grep "Content-Length:"`
						tot_size="${tot_size#*Content-Length: }"
						tot_size="${tot_size%%' '*}"
						if [ "$namef" != "${namef//uploaded.hellip.tmp}" ];then
							hellip_namef="$namef"
							
							namef=`cat $path_tmp/${hellip_namef}_stdout.tmp |grep filename`
							namef="${namef#*'filename="'}"
							namef="${namef%'"'*}"
							name_stdout="$path_tmp/${hellip_namef}_stdout.tmp"
						fi
					elif [ "$dlr" == "Axel" ]; then
						tot_size=`cat "$path_tmp/head_stdout" |grep 'File size'`
						tot_size="${tot_size#*File size: }"
						tot_size="${tot_size%% *}"
					fi
					
					sizef=`ls -l "$namef" 2>/dev/null| awk '{ print($5) }'`
					sizehellip=`ls -l "$hellip_namef" 2>/dev/null| awk '{ print($5) }'`
					[ -z "$sizehellip" ] && sizehellip=0
					[ -z "$sizef" ] && sizef=0
					
					if ( [ ! -z "$sizef" ] || [ ! -z "$sizehellip" ] ) && [ ! -z "$tot_size" ]; then
						
						if ( [ "$nomefile" == "$namef" ] || [ "$nomefile" == "$hellip_namef" ] ) && ( (( $tot_size>$sizef )) && (( $tot_size>$sizehellip )) ); then
						
							rm -f "$nomefile"
# 							read -p "ckloop !"
							links_loop + "$indirizzo"
						else
							links_loop - "$indirizzo"1
						fi
					else
						links_loop - "$indirizzo"
					fi
					unset sizef sizehellip
				fi
			done
		fi
	fi
}


if [ -f "$conf" ]; then
	downloader=`cat $conf`
else
	downloader=Axel
fi
################################### HEADER
#separator z
header
separator z
###################################
for arg in $@ ; do
	case "$arg" in
		-i|--interactive)
			interactive
			;;
		-clean)
			( rm -r "$path_tmp"/* 2>/dev/null && print_c 1 "File temporanei cancellati" ) || print_c 3 "Pulizia file temporanei non effettuata (file inesistenti)"
			;;
		-u | --update)
			print_c 1 "Aggiornamento di $PROG ..."
			wget -T $max_waiting "$url_update" -O /tmp/$prog 
			print_c 1 "Installazione di $PROG in /usr/local/bin/"
			( mv /tmp/$prog /usr/local/bin/$prog ; chmod +x /usr/local/bin/$prog ; print_c 1 "Aggiornamento completato." ) || ( sudo mv /tmp/$prog /usr/local/bin/$prog ; sudo chmod +x /usr/local/bin/$prog ; print_c 1 "Aggiornamento completato." ) || ( echo -n "(Root)" ; su -c "mv /tmp/$prog /usr/local/bin/$prog ; chmod +x /usr/local/bin/$prog" ; print_c 1 "Aggiornamento completato." ) || ( print_c 3 "Aggiornamento automatico non riuscito" )
			echo "which ${prog}:"
			which $prog
			echo
			;;
		wget)
			downloader=Wget
			;;
		axel)
			downloader=Axel
			;;
		
		-m | --multi)
			multi=1
# 			if [ -e "/cygdrive" ]; then
# 				export DISPLAY=:0
# 				X &>/dev/null &
# 				exec aewm++ &>/dev/null &
# 			fi
			;;
		
		-h | --help)
			usage
			;;
			
		-c | --configure)
			configure
			;;
		
		*)
			if [ -f "$arg" ]; then
				file="$arg"
			else
				print_c 3 "Il file $arg non esiste"
				echo
				usage
			fi
			;;
	esac
done


if [ "$downloader" == "Axel" ]; then
	while [ -z "`which axel 2>/dev/null`" ]; do
		clear
		print_c 3 "ATTENZIONE: Axel non è installato nel tuo sistema"
		
		echo -e "$PROG può scaricare con Wget ma raccomanda fortemente Axel, perché:\n
	- può accelerare sensibilmente il download
	- permette il recupero dei download in caso di interruzione
	
Per ulteriori informazioni su Axel: http://alioth.debian.org/projects/axel/

1) Installa automaticamente Axel da pacchetti
2) Installa automaticamente Axel da sorgenti
3) Esci da $PROG per installare Axel manualmente (puoi trovarlo qui: http://pkgs.org/search/?keyword=axel)
4) Ignora Axel e continua con Wget
5) Configura Wget di default
6) Ripristina la condizione iniziale (Axel di default)"

		print_c 2 "Scegli cosa fare (1-6)"
		read input
		
		case $input in
		
		1) install_pk ;;
		2) install_src ;;
		3) exit ;;
		4) downloader=Wget ; break ;;
		5) echo Wget > $conf ;;
		6) rm $conf ;;
		
		esac
	done
fi


print_c 1 "Download con $downloader"


if [ -f "$file_log" ]; then
	log=1
fi

if [ "$file" == "" ] ; then 
	print_c 2 "Incolla la lista dei link (nowdownload, mediafire, uploaded, ul, easybytez) poi digita @" 
	echo "[per uscire da $PROG digita ctrl+c]"
	read -d @ -a links
	printf "%s\n" "${links[@]}" > links.txt
	file="links.txt"
	echo
fi



#converti UL.TO in UPLOADED.TO/FILE
lnks=`cat $file`
rm $file

echo -e "${lnks// /\n}" | while read line ; do 
	if [ "$line" != "${line//http:\/\/ul.to/}" ]; then
		line=${line//ul.to/uploaded.to/file}
		
		
	fi
	echo $line >> $file
done


while true; do
	if [ ! -z "$file" ]; then	
		stop=`wc -l "$file" | awk '{ print($1) }'`
		stop=$(( $stop+1 ))
		riga=1
		
		while [ $riga != $stop ]; do
			[ ! -z "$pid_dl" ] && check_freespace
			unset pid_dl checked
			#cancella file temporanei per recupero nome da uploaded di file incompleti per download interrotto cancellati dalla directory
			ls *.uploaded.hellip.tmp 2>/dev/null | while read file_tmp; do
				fsize_tmp=`ls -s --block-size=K "$file_tmp" | awk '{ print($1) }'`
				fsize_tmp=${fsize_tmp//K/}
				#echo $fsize_tmp
				if (( $fsize_tmp>10 )); then
					rm -f "$file_tmp"
				else
					nfile_tmp=`cat "$file_tmp"`
					nfile_tmp="${nfile_tmp#*'nomefile:'}"
					
					if [ ! -f "$nfile_tmp" ]; then
						rm -f "$file_tmp"
					fi
				fi
			done
			#while [ ! -z "$indirizzo" ];do
				indirizzo=`less $file |sed -n "${riga}p"`
				(( riga++ ))
			#fi
			url=${indirizzo//\?*}
			mkdir $path_tmp 2>/dev/null
			echo > $path_tmp/zdl.tmp > $path_tmp/zdl2.tmp
			links_loop - $indirizzo
			echo
			echo
			separator "Z"
			
			echo -e "\nLink da processare: $indirizzo"
			
	# 		if [ "$url" != "${url//4us.to/}" ]; then
	# 			
	# 			## 4US.TO ###############
	# 			get_tmps		
	# 			link=`cat $path_tmp/zdl.tmp | grep morpheus `
	# 			link=${link#*document.location=\"}
	# 			link=${link%\"*}
	# 			
	# 			nomefile=${link##*=} 
	# 			n=1
	# 	
			if [ "$url" != "${url//mediafire.}" ]; then
				
				# MEDIAFIRE.COM ##############
				check_ip mediafire
				
				get_tmps
				link=`less $path_tmp/zdl.tmp |grep 'kNO = '`
				link=${link#*'kNO = "'}
				link=${link//\" onclick=\"avh*/}
				link=${link%'"'*}
				
				nomefile=${link##*'/'}
				n=4
			elif [ "$url" != "${url//nowdownload.}" ]; then
				
				## http://www.nowdownload ###########
				
				get_tmps
				token=`less $path_tmp/zdl.tmp | grep token`
				token=${token//*=}
				token=${token//\"*/}
				prelink=${indirizzo//\/dl/\/dl2}"/$token"
				print_c 2 "Attendi circa 30 secondi:"
				k=`date +"%s"`
				s=0
				unset link
				while true; do

					if (( $s>29 )); then
						wget -t 1 -T $max_waiting --load-cookies=$path_tmp/cookies.zdl -O "$path_tmp/zdl2.tmp" "$prelink" &>/dev/null 
					fi
					sleep 1
					s=`date +"%s"`
					s=$(( $s-$k ))
					
					echo -e $s"\r\c"
					
					link=`cat $path_tmp/zdl2.tmp |grep "Click here to download" 2>/dev/null`
					link=${link//*href=\"} 
					link=${link//\"*}

					[ ! -z "$link" ] && break
				done	
				nomefile1=`cat $path_tmp/zdl.tmp | grep 'Downloading'`
				nomefile1="${nomefile1#*'<br> '}"
				nomefile1="${nomefile1%%</h4>*}"
				nomefile1="${nomefile1%' '*}"
				nomefile1="${nomefile1%' '*}"
				nomefile1="${nomefile1%' '*}"
				nomefile1="${nomefile1//'<br>'/}"
				while [ "$nomefile1" != "${nomefile1%.}" ]; do
					nomefile1=${nomefile1%.}
				done
				nomefile2="${link//*\//}"
				nomefile2="${nomefile2#*_}"
				
				if [ "$nomefile2" != "${nomefile2//$nomefile1}" ]; then
					nomefile="$nomefile2"
				else
					nomefile="$nomefile1"
				fi
	
	# 		elif [ "$url" != "${url//sharpfile.}" ]; then
	# 			wget $indirizzo -O $path_tmp/zdl.tmp &>/dev/null
	# 			tmp="$path_tmp/zdl.tmp"
	# 			input_hidden
	# 			
	# 			if ( [ ! -f "${nomefile}.st" ] && [ -f "${nomefile}" ] && [ "$downloader" = "Axel" ] ) || ( [ -f "${nomefile}" ] && [ "$downloader" = "Wget" ] ); then
	# 				echo -n
	# 			else
	# 				check_ip sharpfile
	# 				wget --keep-session-cookies --save-cookies=$path_tmp/cookies.zdl $indirizzo -O $path_tmp/zdl.tmp &>/dev/null
	# 				echo -e "...\c"
	# 				tmp="$path_tmp/zdl.tmp"
	# 				input_hidden
	# 				
	# 				post_data="${post_data//'op=catalogue&'}"
	# 				
	# 				wget --load-cookies=$path_tmp/cookies.zdl --keep-session-cookies --save-cookies=$path_tmp/cookies2.zdl --post-data="$post_data&method_free=Free Download" $indirizzo -O $path_tmp/zdl2.tmp &>/dev/null
	# 				
	# 				captcha_html=`cat $path_tmp/zdl2.tmp |grep "position:absolute;padding-left"`
	# 				unset post_data
	# 				unset ascii_dec
	# 				unset i
	# 				while [ ${#ascii_dec[*]} != 4 ];do
	# 					captcha_html="${captcha_html#*'position:absolute;padding-left:'}"
	# 					i="${captcha_html%%px*}"
	# 					captcha_html="${captcha_html#*'&#'}"
	# 					ascii_dec[$i]="${captcha_html%%';'*}"
	# 				done
	# 				pseudo_captcha
	# 				print_c 2 "Attendi:"
	# 				
	# 				code=${captcha[*]}
	# 				code=${code// /}
	# 				
	# 				s=65
	# 				while [ $s != 0 ]; do
	# 					echo -e "  \r\c"
	# 					echo -e $s"\r\c"
	# 					sleep 1
	# 					(( s-- ))
	# 				done
	# 				echo -e "  \r\c"
	# 				tmp="$path_tmp/zdl2.tmp"
	# 				input_hidden
	# 				post_data="${post_data//'op=catalogue&'}"
	# 				post_data="${post_data}&code=${code}"
	# 			fi
	# 			link="$indirizzo"
	
	# 		elif [ "$url" != "${url//shareflare.}" ]; then
	# 			
	# 			## http://www.shareflare.net ####
	# 			check_ip shareflare
	# 			get_tmps
	# 			tmp="$path_tmp/zdl.tmp"
	# 			input_hidden
	# 			if [ ! -f "${nomefile}.st" ] && [ -f "${nomefile}" ]; then
	# 					echo
	# 			else
	# 				wget --load-cookies=$path_tmp/cookies.zdl --keep-session-cookies --save-cookies=$path_tmp/cookies.zdl --post-data=$post_data"&submit_ifree=Download file" http://shareflare.net/download4.php -O $path_tmp/download4.tmp &>/dev/null
	# 				echo -e "...\c"
	# 				unset post_data
	# 				
	# 				input_hidden
	# 				wget --load-cookies=$path_tmp/cookies.zdl --post-data=$post_data"&frameset=Download file." http://shareflare.net/download3.php -O $path_tmp/download3.tmp &>/dev/null
	# 				echo -e "...\c"
	# 				
	# 				wget --load-cookies=$path_tmp/cookies.zdl --post-data=$post_data"&frameset=Download file." "http://shareflare.net/tmpl/tmpl_frame_top.php?link=" -O $path_tmp/tmpl_frame_top.php &>/dev/null
	# 				echo -e "...\c"
	# 				
	# 				print_c 2 "Attendi circa 45 secondi:"
	# 				
	# 				k=`date +"%s"`
	# 				while [ "$goal" == "" ]; do
	# 					sleep 1
	# 					s=`date +"%s"`
	# 					s=$(( $s-$k ))
	# 					echo -e $s"\r\c"
	# 					
	# 					if (( $s>40 )); then
	# 						wget --load-cookies=$path_tmp/cookies.zdl --post-data=$post_data"&frameset=Download file." "http://shareflare.net/tmpl/tmpl_frame_top.php?link=" -O $path_tmp/tmpl_frame_top.php &>/dev/null
	# 						
	# 						goal=`less $path_tmp/tmpl_frame_top.php |grep direct_link_0` #grep "========================="`
	# 						sleep 1
	# 					fi
	# 					if (( $s>90 )); then
	# 						break
	# 					fi
	# 				done
	# 				
	# 				link=${goal#*\"}
	# 				link=${link//\"*/}
	# 				
	# 				
	# 				n=1
	# 			fi
	# 			
			elif [ "$url" != "${url//easybytez.}" ]; then
				indirizzo=`wget -t 1 -T $max_waiting -O - $indirizzo -q |grep 'You have requested'` #&>/dev/null
				indirizzo="${indirizzo##*'<font color="red">'}"
				indirizzo="${indirizzo%%'</font'*}"
				#indirizzo="${indirizzo// /}"
				nomefile=${indirizzo##*\/}
				not_available=`wget -T $max_waiting -q -O - $indirizzo |grep "File not available"`
				
				if [ ! -z "${nomefile}" ] && [ ! -f "${nomefile}" ] && [ -z "$not_available" ]; then
					check_ip easybytez
					
					wget -q -t 1 -T $max_waiting --retry-connrefused --keep-session-cookies --save-cookies=$path_tmp/cookies.zdl -O "$path_tmp/zdl.tmp" $indirizzo &>/dev/null
					echo -e "...\c"
					
					tmp="$path_tmp/zdl.tmp"
					input_hidden
				
					wget -t 1 -T $max_waiting -q --load-cookies=$path_tmp/cookies.zdl --keep-session-cookies --save-cookies=$path_tmp/cookies.zdl --post-data="${post_data}&method_free=Free Download" $indirizzo -O $path_tmp/zdl2.tmp &>/dev/null
					echo -e "...\c"
					exceeded=`cat $path_tmp/zdl2.tmp |grep "Upgrade your account to download bigger files"`
					unset post_data
					if [ -z "$not_available" ] && [ -z "$exceeded" ]; then
				
						tmp="$path_tmp/zdl2.tmp"
						input_hidden
		
						wget -t 1 -T $max_waiting -q --load-cookies=$path_tmp/cookies.zdl --keep-session-cookies --save-cookies=$path_tmp/cookies.zdl --post-data="${post_data}&btn_download=Download File" $indirizzo -O $path_tmp/zdl3.tmp &>/dev/null
						echo -e "...\c"
						unset post_data
					
						print_c 2 "Attendi circa 60 secondi:"
						for s in `seq 0 60`; do
							echo -e $s"\r\c"
							sleep 1
						done
						echo -e "  \r\c"
						tmp="$path_tmp/zdl3.tmp"
						input_hidden
						
						
						link="$indirizzo"
						post_data="${post_data}&btn_download=Download File"
					fi
				fi
					
			elif [ "$url" != "${url//uploaded.}" ]; then
				unset hellip
				indirizzo="${indirizzo%/}"
				wget -t 1 -T $max_waiting $indirizzo -q -O $path_tmp/test_page.tmp &>/dev/null
				test_exceeded=`cat $path_tmp/test_page.tmp |grep 'small style'`
				test_exceeded="${test_exceeded#*'>'}"
				test_exceeded="${test_exceeded%'<'*}"
				test_exceeded=`echo $test_exceeded |grep GB`
				if [ ! -z "$test_exceeded" ]; then
					test_exceeded=${test_exceeded%' '*}
					test_exceeded=${test_exceeded//,/.}
					test_exceeded=`echo "( $test_exceeded>1 )" |bc -l 2>/dev/null`
				fi
				#not_available=`cat $path_tmp/test_page.tmp |grep 'Page not found'`
				test_available=`wget -q -O - -t 1 -T $max_waiting $indirizzo |grep "</html"`
	
				if [ "$test_exceeded" == "1" ]; then
					exceeded=1
				elif [ -z "$test_available" ]; then
					not_available=true
				else
					if [ "${indirizzo##*/}" != "${indirizzo//*file\/}" ]; then
						nomefile=${indirizzo##*/}
						indirizzo2=${indirizzo%/*}
						file_id=${indirizzo2##*/}
					else 
						wget -t 1 -T $max_waiting "$indirizzo" -O $path_tmp/zdl.tmp &>/dev/null
						file_id=${indirizzo##*/} 
						nomefile=`cat $path_tmp/zdl.tmp |grep "id=\"filename\""`
						nomefile="${nomefile//<\/a*/}"
						nomefile="${nomefile##*>}"
						if [ "$nomefile" != "${nomefile//'&hellip;'/}" ]; then
							nomefile="${file_id}_${nomefile//'&hellip;'/.}.uploaded.hellip.tmp"
							#hellip=true
						fi
						
					fi
					if [ ! -f "$nomefile" ]; then
						check_ip uploaded
						wget -t 1 -T $max_waiting --keep-session-cookies --save-cookies=$path_tmp/cookies.zdl "$indirizzo" -O $path_tmp/zdl.tmp &>/dev/null
						echo -e "...\c"
						
						cooking=`cat $path_tmp/zdl.tmp |grep ref_user`
						cooking="${cooking//*\(\'/}"
						cooking=${cooking//"'"*/}
						
						echo "uploaded.to     FALSE   /       FALSE   0       ref     $cooking" >> $path_tmp/cookies.zdl
						
						wget -t 1 -T $max_waiting --load-cookies=$path_tmp/cookies.zdl "http://uploaded.to/io/ticket/captcha/$file_id" -O "$path_tmp/goal.tmp" &>/dev/null
						echo -e "...\c"
						
						link=`cat $path_tmp/goal.tmp`
						link=${link//*url:\'/}
						link=${link//\'*}
					fi
				fi
				
			else
				[ "$url" != "" ] && print_c 3 "$url non supportato da $PROG"
				not_available=1
			fi
			nomefile="${nomefile// /_}"
			
			if [ "$url" != "${url//uploaded.}" ] || [ "$url" != "${url//easybytez.}" ];then # || [ "$url" != "${url//sharpfile.}" ]; then
				if [ "$downloader" == "Axel" ]; then
					dler=$downloader
					downloader=Wget
					ch_dler=1
					print_c 3 "Il server non permette l'uso di $dler: il download verrà effettuato con $downloader"
				fi
			fi
			
# 			check_dl
# 			unset checked
			#### DOWNLOAD ####
			
			if [ ! -f "${nomefile}.st" ] && [ -f "${nomefile}" ] && [ "$downloader" = "Axel" ]; then
				_log 1
				no_newip=true
			elif ( [ -f "${nomefile}" ] || [ -f "${path_tmp}/${nomefile}" ] ) && [ "$downloader" = "Wget" ]; then
				_log 1
				no_newip=true
				check_loop
			elif [ ! -z "$not_available" ]; then
				[ "$indirizzo" != "" ] && _log 3
				no_newip=true
			elif [ ! -z "$exceeded" ]; then
				_log 4
				no_newip=true
			elif [ "$link" == "" ] || [ "${nomefile}" == "" ]; then
				_log 2
				unset no_newip
				#no_newip=true
				#old_ip
			elif [ "$link" != "${link//{\"err\"/}" ]; then
				_log 2
				unset no_newip
	# 		elif [ "${nomefile}" == "" ]; then
	# 			_log 5
	# 			unset no_newip
			elif [ "$indirizzo" != "" ]; then
				if [ "$downloader" = "Axel" ]; then
					export AXEL_COOKIES=$path_tmp/cookies.zdl
					print_c 1 "downloading --> $nomefile ..."
					[ "$nomefile" != "" ] && argout="-o"
					# -a tolto
					axel -n $n "$link" $argout "$nomefile" >> "$path_tmp/${nomefile}_stdout.tmp" & 
					pid_dl=$!
					echo -e "${pid_dl}\nlink_${prog}: $indirizzo\nAxel" > "$path_tmp/${nomefile}_stdout.tmp"
					pids_alive[${#pids_alive[*]}]=${pid_dl}
					unset post_data
					
				elif [ "$downloader" = "Wget" ]; then
					print_c 1 "downloading --> $nomefile ..."
					[ "$nomefile" != "" ] && argout="-O" && fileout="$nomefile"
					[ "$url" != "${url//easybytez.}" ] && unset argout fileout
					LANG='en_US.UTF-8' wget -t 1 -T $max_waiting --retry-connrefused -c -nc --load-cookies=$path_tmp/cookies.zdl --keep-session-cookies --save-cookies=$path_tmp/cookies.zdl  --post-data="${post_data}" "$link" -S $argout "$fileout" --progress=bar:force -a "$path_tmp/${nomefile}_stdout.tmp" &
					pid_dl=$!
					echo -e "${pid_dl}\nlink_${prog}: $indirizzo\nWget" > "$path_tmp/${nomefile}_stdout.tmp"
					pids_alive[${#pids_alive[*]}]=${pid_dl}
					unset post_data
				fi
				
				if [ $multi == 1 ] && [ "$url" == "${url//nowdownload.}" ]; then
					count_down=$(( $max_waiting+5 ))
				else	
					count_down=$(( $max_waiting+5 ))
				fi
				print_c 2 "Attendi:"
				while [ "$count_down" != 0 ]; do
					echo -e "  \r\c"
					echo -e $count_down"\r\c"
					sleep 1
					(( count_down-- ))
				done
				echo -e "  \r\c"
				
				####################################
				# check free space
				if [ ! -z "$pid_dl" ]; then
					name_stdout=`ls -1 "$path_tmp/${nomefile}_stdout.tmp"`
					dlr=`head -n 3 "$name_stdout" | sed -n '3p'`
			
					if [ "$dlr" == "Wget" ]; then
						tot_size=`cat "$name_stdout" |grep "Content-Length:"`
						tot_size="${tot_size#*Content-Length: }"
						tot_size="${tot_size%%' '*}"
						#sleep 2
					elif [ "$dlr" == "Axel" ]; then
						tot_size=`cat "$name_stdout" |grep 'File size'`
						tot_size="${tot_size#*File size: }"
						tot_size="${tot_size%% *}"
					fi
					
					check_freespace
				fi
				####################################
				# hellip
				if [ -f "$nomefile" ] && [ "$nomefile" != "${nomefile//.uploaded.hellip.tmp}" ]; then
					unset quit newname pid 
					newname=`cat $path_tmp/${nomefile}_stdout.tmp |grep filename`
					newname="${newname#*'filename="'}"
					newname="${newname%'"'*}"
					if [ "$newname" != "" ]; then
						check_dl
						
						if [ $? == 5 ]; then
							kill_dl
							rm -f  "$nomefile"
						elif [ ! -f "$newname" ]; then
							mv "$nomefile" "$newname"
							print_c 1 "Recupero nomefile: $nomefile rinominato come $newname"
						fi
						echo "$pid_prog - nomefile:$newname" > "$path_tmp/$nomefile"
						unset checked
					fi
				fi
				unset no_newip
			fi
			if [ -z $no_newip ]; then
				[ "$url" != "${url//mediafire.}" ] && newip[${#newip[*]}]=mediafire
				[ "$url" != "${url//uploaded.}" ] && newip[${#newip[*]}]=uploaded
		# 		[ "$url" != "${url//shareflare.}" ] && newip[${#newip[*]}]=shareflare
				[ "$url" != "${url//easybytez.}" ] && newip[${#newip[*]}]=easybytez
	# 			[ "$url" != "${url//sharpfile.}" ] && newip[${#newip[*]}]=sharpfile
			fi
			[ "$ch_dler" == "1" ] && downloader=$dler && unset ch_dler
	
			noproxy
			if [ $multi == 1 ]; then 
				check_dl
				checked=1
			fi
			while [ -z "$checked" ]; do #nomefile
				check_dl
				sleep 3
			done
			
			sleep 1
			unset link post_data goal not_available exceeded
		done
	fi
	if [ -f "$path_tmp/links_loop.txt" ]; then
		file="$path_tmp/links_loop.txt"
		
	else
		unset file
		while true; do 
			check_dl
			sleep 3
			if [ -f "$path_tmp/links_loop.txt" ]; then
				file="$path_tmp/links_loop.txt"
				break
			fi
			nodl=0
			for pk in ${killed[*]}; do
				check_tmps=`head -n 1 $path_tmp/*_stdout.tmp 2>/dev/null`
				[ "$check_tmps" != "${check_tmps//$pk}" ] || (( nodl++ ))
			done
			[ $nodl == 0 ] && break
		done
	fi
	## DEBUG
# 	[ ! -f "$path_tmp/links_loop.txt" ] && echo "finito loop"
# 	( [ "${pids_alive[*]}" == "" ] && echo "finito pids" ) || echo "pids: ${pids_alive[*]}"
	##

	[ ! -f "$path_tmp/links_loop.txt" ] && [ "${pids_alive[*]}" == "" ] && break
done

separator "-"
print_c 1 "Tutti i link per il download sono stati processati."
separator "-"

#test_cleaning=`ls $path_tmp/*_stdout.tmp 2>/dev/null`
#[ "$test_cleaning" == "" ] && rm -r "$path_tmp"
rm -r "$path_tmp"

echo
echo
if [ -f "$file_log" ]; then
	print_c 3 "In questa directory è presente un file che contiene un elenco di operazioni di $PROG terminate senza successo."
	echo -e "Per leggerlo, digita:\n\n cat $file_log\n\n"
fi


exit
